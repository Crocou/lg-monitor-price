# crawl_scroll_zip65760.py
"""
Amazon.de ë² ìŠ¤íŠ¸ì…€ëŸ¬ â–¸ Monitors 1~100ìœ„
- LG ëª¨ë‹ˆí„° í•„í„°, ê°€ê²©Â·ìˆœìœ„Â·ë³€ë™ ê¸°ë¡ (ìŠ¤í¬ë¡¤ í¬í•¨)
- â˜… ë°°ì†¡ì§€(ìš°í¸ë²ˆí˜¸) 65760 ê³ ì • (UI í´ë¦­ ë°©ì‹)
"""

import sys, os, re, json, base64, datetime, time, logging
import pandas as pd, gspread, pytz
from google.oauth2.service_account import Credentials
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import (
    NoSuchElementException,
    StaleElementReferenceException,
    TimeoutException,
)
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from gspread_formatting import format_cell_ranges, CellFormat, TextFormat, Color

# â”€â”€â”€ ì„¤ì • ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_URL = "https://www.amazon.de/gp/bestsellers/computers/429868031/"
CARD_SEL = "li.zg-no-numbers"

# â”€â”€â”€ 0. ë¡œê¹… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.FileHandler("crawl_cards.log", encoding="utf-8"),
              logging.StreamHandler(sys.stdout)],
)
logging.info("ğŸ” LG ëª¨ë‹ˆí„° í¬ë¡¤ëŸ¬ ì‹œì‘")

# â”€â”€â”€ 1. Selenium â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_driver():
    opt = webdriver.ChromeOptions()
    opt.add_argument("--headless=new")
    opt.add_argument("--no-sandbox")
    opt.add_argument("--disable-dev-shm-usage")
    opt.add_argument("--window-size=1280,4000")
    opt.add_argument("--lang=de-DE")
    opt.add_argument(
        "user-agent=Mozilla/5.0 (X11; Linux x86_64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0 Safari/537.36"
    )
    return webdriver.Chrome(options=opt)

# â˜… 1-A. ìš°í¸ë²ˆí˜¸ë¥¼ UIë¡œ ì„¤ì • --------------------------------------
def set_zip_ui(driver, zip_code: str = "65760", timeout: int = 30):
    """UI í´ë¦­ ë°©ì‹ìœ¼ë¡œë§Œ ìš°í¸ë²ˆí˜¸ë¥¼ ê°•ì œ ì„¤ì •í•œë‹¤.
       í›„ë³´ id ë‹¤ì¤‘ ì‹œë„, ì‹¤íŒ¨ ì‹œ TimeoutException ê·¸ëŒ€ë¡œ throw.
    """
    wait = WebDriverWait(driver, timeout)
    wait.until(lambda d: d.execute_script("return document.readyState") == "complete")
    logging.info("ğŸ“¦ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ, ìš°í¸ë²ˆí˜¸ ì„¤ì • ì‹œì‘ (%s)", zip_code)

    # 0) ì¿ í‚¤ ë°°ë„ˆ ë‹«ê¸°(ìˆì„ ë•Œë§Œ)
    try:
        logging.info("ğŸ” ì¿ í‚¤ ë°°ë„ˆ í™•ì¸")
        wait.until(EC.element_to_be_clickable((By.ID, "sp-cc-accept"))).click()
        driver.execute_script("window.scrollTo(0, 0)")
        logging.info("âœ… ì¿ í‚¤ ë°°ë„ˆ ë‹«í˜")
    except TimeoutException:
        logging.info("â„¹ï¸ ì¿ í‚¤ ë°°ë„ˆ ì—†ìŒ ë˜ëŠ” ì´ë¯¸ ë‹«í˜")

    # 1) ìœ„ì¹˜ ì„ íƒ ë²„íŠ¼ í´ë¦­
    logging.info("ğŸ“ ìœ„ì¹˜ ì„¤ì • ë²„íŠ¼ í´ë¦­ ì‹œë„")
    wait.until(EC.element_to_be_clickable((By.ID, "nav-global-location-data-modal-action"))).click()
    logging.info("âœ… ìœ„ì¹˜ ì„¤ì • íŒì—… ì—´ë¦¼")

    # 2) ìš°í¸ë²ˆí˜¸ ì…ë ¥
    logging.info("âŒ¨ï¸ ìš°í¸ë²ˆí˜¸ ì…ë ¥ë€ ì°¾ëŠ” ì¤‘")
    input_el = wait.until(EC.presence_of_element_located((By.ID, "GLUXZipUpdateInput")))
    input_el.clear()
    input_el.send_keys(zip_code)
    logging.info("âœ… ìš°í¸ë²ˆí˜¸ ì…ë ¥ ì™„ë£Œ")

    # 3) Apply í´ë¦­
    logging.info("ğŸŸ¡ 'Apply' ë²„íŠ¼ í´ë¦­ ì‹œë„")
    wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id="GLUXZipUpdate"]/span/input'))).click()
    logging.info("âœ… 'Apply' ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")

    # 4) ë‹«ê¸° ë²„íŠ¼ í´ë¦­
    logging.info("ğŸŸ¡ 'Confirm Close' ë²„íŠ¼ í´ë¦­ ì‹œë„")
    wait.until(EC.element_to_be_clickable((By.ID, "GLUXConfirmClose"))).click()
    logging.info("âœ… ìœ„ì¹˜ ì„¤ì • íŒì—… ë‹«í˜")

    # 5) ìµœì¢… í™•ì¸
    logging.info("ğŸ” í—¤ë”ì— ìš°í¸ë²ˆí˜¸ ë°˜ì˜ í™•ì¸ ì¤‘")
    wait.until(lambda d: zip_code in d.find_element(By.ID, "glow-ingress-line2").text)
    logging.info("ğŸ¯ ìš°í¸ë²ˆí˜¸ %s UI ë°©ì‹ ì ìš© ì„±ê³µ", zip_code)


# â”€â”€â”€ 2. ì¹´ë“œ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_cards_and_parse(page: int, driver):
    parsed_items = []

    url = BASE_URL if page == 1 else f"{BASE_URL}?pg={page}"
    logging.info("â–¶ï¸  ìš”ì²­ URL (page %d): %s", page, url)
    driver.get(url)

    # ìµœì†Œ í•œ ì¥ì´ë¼ë„ ë Œë” ëŒ€ê¸°
    try:
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, CARD_SEL))
        )
    except TimeoutException:
        logging.error("â›” page %d: ì¹´ë“œ 0ê°œ - íƒ€ì„ì•„ì›ƒ", page)
        return []

    # ìŠ¤í¬ë¡¤ ë¡œë”©
    start = time.time()
    last, SCROLL_PAUSE, MAX_WAIT = 0, 3, 60
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(SCROLL_PAUSE)
        now = len(driver.find_elements(By.CSS_SELECTOR, CARD_SEL))
        if page == 1 and now < 50 and time.time() - start < MAX_WAIT:
            continue
        if now == last or time.time() - start >= MAX_WAIT:
            break
        last = now

    cards = driver.find_elements(By.CSS_SELECTOR, CARD_SEL)
    logging.info("âœ… page %d ì¹´ë“œ %dê°œ", page, len(cards))

    # ì¹´ë“œ ë£¨í”„
    for idx, card in enumerate(cards, 1):
        try:
            rank = int(re.sub(r"\D", "", card.find_element(
                By.XPATH, './/span[contains(@class,"zg-bdg-text")]').text))
        except (NoSuchElementException, ValueError, StaleElementReferenceException):
            continue

        try:
            title = card.find_element(
                By.XPATH, './/div[contains(@class,"_cDEzb_p13n-sc-css-line-clamp-2_EWgCb")]'
            ).text.strip()
        except NoSuchElementException:
            title = card.find_element(By.XPATH, './/img[@alt]').get_attribute("alt").strip()

        lg_match = bool(re.search(r"\bLG\b", title.replace("\u00a0", " "), re.I))

        # â”€â”€â”€ ê°€ê²© â”€â”€â”€
        price_raw = card.find_element(
            By.CSS_SELECTOR, 'span._cDEzb_p13n-sc-price_3mJ9Z'
        ).text.strip()                         # ì—†ìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ â†’ í¬ë¡¤ë§ ì¤‘ë‹¨

        if not price_raw:                      # ë¹ˆ ë¬¸ìì—´ì´ë©´ offers ë¬¸êµ¬
            try:
                offer_txt = card.find_element(
                    By.CSS_SELECTOR, 'span.a-color-secondary').text.strip()
                m = re.search(r'â‚¬[\d\.,]+', offer_txt)
                if m:
                    price_raw = m.group(0)
            except NoSuchElementException:
                pass

        # â”€â”€â”€ ë§í¬/ASIN â”€â”€â”€
        try:
            href = card.find_element(By.XPATH, './/a[contains(@href,"/dp/")]').get_attribute("href")
            link = href.split("?", 1)[0] if href.startswith("http") else "https://www.amazon.de" + href.split("?", 1)[0]
            asin = re.search(r"/dp/([A-Z0-9]{10})", link).group(1)
        except Exception:
            continue

        if lg_match:
            parsed_items.append({
                "asin": asin,
                "title": title,
                "url": link,
                "price": price_raw,   # â˜… strip ê²°ê³¼ ê·¸ëŒ€ë¡œ ì €ì¥
                "rank": rank,
            })

    return parsed_items

# â”€â”€â”€ 3. í¬ë¡¤ë§ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
driver = get_driver()
try:
    driver.get("https://www.amazon.de/")
    set_zip_ui(driver, "65760")          # â˜… UI ë°©ì‹ ìš°í¸ë²ˆí˜¸ ì„¤ì •

    items = []
    for pg in (1, 2):
        items += fetch_cards_and_parse(pg, driver)
finally:
    driver.quit()

logging.info("LG ëª¨ë‹ˆí„° í•„í„° í›„ %dê°œ ë‚¨ìŒ", len(items))

# â”€â”€â”€ 4. DataFrame & Google Sheet (ê°€ê²©ì€ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cols = ["asin", "title", "url", "price", "rank"]
df_today = pd.DataFrame(items, columns=cols)

if df_today.empty:
    logging.info("LG ëª¨ë‹ˆí„° ì—†ìŒ â†’ ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ìƒëµ")
    sys.exit(0)

df_today = df_today.sort_values("rank").reset_index(drop=True)
kst = pytz.timezone("Asia/Seoul")
df_today["date"] = datetime.datetime.now(kst).strftime("%Y-%m-%d %H:%M:%S")

# price_delta ë“± ìˆ«ì ê³„ì‚°ì€ ìƒëµí•˜ê±°ë‚˜ í•„ìš”í•˜ë©´ ë³„ë„ íŒŒì‹± í›„ ì§„í–‰

# â”€â”€â”€ 5. Google Sheet ê¸°ë¡ (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼, price ì»¬ëŸ¼ì€ ë¬¸ìì—´) â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
creds = Credentials.from_service_account_info(
    json.loads(base64.b64decode(os.environ["GCP_SA_BASE64"]).decode()),
    scopes=SCOPES,
)
gc = gspread.authorize(creds)
SHEET_ID = os.environ["SHEET_ID"]
sh = gc.open_by_key(SHEET_ID)

ws_hist = sh.worksheet("History") if "History" in [w.title for w in sh.worksheets()] else sh.add_worksheet("History", rows=2000, cols=20)
ws_today = sh.worksheet("Today") if "Today" in [w.title for w in sh.worksheets()] else sh.add_worksheet("Today", rows=100, cols=20)

if not ws_hist.get_all_values():
    ws_hist.append_row(cols + ["date"], value_input_option="USER_ENTERED")
ws_hist.append_rows(df_today[cols + ["date"]].values.tolist(), value_input_option="USER_ENTERED")

ws_today.clear()
ws_today.update([cols + ["date"]] + df_today[cols + ["date"]].values.tolist(), value_input_option="USER_ENTERED")

logging.info("Google Sheet ì—…ë°ì´íŠ¸ ì™„ë£Œ â€” LG ëª¨ë‹ˆí„° %dê°œ", len(df_today))
print("âœ“ Google Sheet ì—…ë°ì´íŠ¸ ì™„ë£Œ â€” LG ëª¨ë‹ˆí„°", len(df_today))
