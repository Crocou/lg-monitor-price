# crawl_scroll_login.py
"""
Amazon.de ë² ìŠ¤íŠ¸ì…€ëŸ¬ â–¸ Monitors 1~100ìœ„
- LG ëª¨ë‹ˆí„° í•„í„°, ê°€ê²©Â·ìˆœìœ„Â·ë³€ë™ ê¸°ë¡ (ìŠ¤í¬ë¡¤ í¬í•¨)
- â˜… ê³„ì • ê¸°ë³¸ ì£¼ì†Œ ê¸°ì¤€ ë°°ì†¡ì§€ UI ì ìš© (ìš°í¸ë²ˆí˜¸ ì§ì ‘ ì…ë ¥)
- ë™ì  í´ë˜ìŠ¤ ëŒ€ì‹  DOM êµ¬ì¡°Â·í…ìŠ¤íŠ¸ ê¸°ë°˜ ì•ˆì •ì  ì…€ë ‰í„° ì ìš©
- â˜… ë¡œê·¸ì¸ ì ˆì°¨ ì œê±° (ì¿ í‚¤/í”„ë¡œí•„ë¡œ ì´ë¯¸ ë¡œê·¸ì¸ ê°€ì •)
"""

import sys, os, re, json, base64, datetime, time, logging
import pandas as pd, gspread, pytz, numpy as np
from google.oauth2.service_account import Credentials
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import (
    NoSuchElementException,
    StaleElementReferenceException,
    TimeoutException,
)
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from gspread_formatting import format_cell_ranges, CellFormat, TextFormat, Color

# â”€â”€â”€ 0. ë¡œê¹… ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("crawl_cards.log", encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ],
)
logging.info("ğŸ” LG ëª¨ë‹ˆí„° í¬ë¡¤ëŸ¬ ì‹œì‘")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. Selenium ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_driver():
    opts = webdriver.ChromeOptions()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--window-size=1280,4000")
    opts.add_argument("--lang=de-DE")
    opts.add_argument(
        "user-agent=Mozilla/5.0 (X11; Linux x86_64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0 Safari/537.36"
    )
    return webdriver.Chrome(options=opts)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ìƒìˆ˜ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_URL = "https://www.amazon.de/gp/bestsellers/computers/429868031/"
CARDS_XPATH = "//div[contains(@class,'a-cardui') and contains(@class,'_cDEzb_card')]//ol/li"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ìƒí’ˆ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_cards_and_parse(page: int, driver):
    parsed_items = []
    url = BASE_URL if page == 1 else f"{BASE_URL}?pg={page}"
    logging.info(f"â–¶ï¸ ìš”ì²­ URL (page {page}): {url}")
    driver.get(url)

    try:
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.XPATH, CARDS_XPATH))
        )
    except TimeoutException:
        logging.error(f"â›” page {page}: ì¹´ë“œ ì—†ìŒ â€” íƒ€ì„ì•„ì›ƒ")
        return []

    start, last_count = time.time(), 0
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(5)
        cards = driver.find_elements(By.XPATH, CARDS_XPATH)
        curr = len(cards)
        if (page == 1 and curr < 50 and time.time() - start < 60) or (
            curr != last_count and time.time() - start < 60
        ):
            last_count = curr
            continue
        break

    logging.info(f"âœ… page {page} ì¹´ë“œ ìˆ˜ì§‘ ì™„ë£Œ: {len(cards)}ê°œ")

    for idx, card in enumerate(cards, start=1):
        try:
            rank_el = card.find_element(By.XPATH, './/span[contains(text(), "#")]')
            rank = int(re.sub(r"\D", "", rank_el.text.strip()))
        except Exception:
            logging.warning(f"[{idx}] ë­í¬ ì¶”ì¶œ ì‹¤íŒ¨ â†’ ê±´ë„ˆëœ€")
            continue

        try:
            title = card.find_element(
                By.XPATH,
                './/div[contains(@class,"_cDEzb_p13n-sc-css-line-clamp-2_EWgCb")]'
            ).text.strip()
        except NoSuchElementException:
            try:
                title = card.find_element(By.XPATH, './/img[@alt]').get_attribute("alt").strip()
            except Exception:
                title = ""
        title = title.replace("\u00a0", " ").replace("\u202f", " ")
        lg_match = bool(re.search(r"\bLG\b", title, re.I))

        try:
            price_raw = ""
            selectors = [
                ('xpath', './/span[@class="a-offscreen"]'),
                ('css',   'span.a-price > span.a-offscreen'),
                ('xpath', './/*[contains(@class, "price")]'),
                ('css',   'span.p13n-sc-price'),
            ]
            for method, sel in selectors:
                try:
                    txt = (card.find_element(By.XPATH, sel).text
                           if method=='xpath'
                           else card.find_element(By.CSS_SELECTOR, sel).text).strip()
                    if 'â‚¬' in txt:
                        price_raw = txt
                        break
                except NoSuchElementException:
                    continue
            if not price_raw:
                raise NoSuchElementException("ìœ íš¨í•œ ê°€ê²© ìš”ì†Œ ì—†ìŒ")
        except Exception:
            logging.warning(f"[{idx}] ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨ â†’ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´")
            price_raw = ""

        try:
            link_el = card.find_element(By.XPATH, './/a[contains(@href,"/dp/")]')
            href = link_el.get_attribute("href").split("?",1)[0]
            asin = re.search(r"/dp/([A-Z0-9]{10})", href).group(1)
        except Exception:
            logging.warning(f"[{idx}] ë§í¬/ASIN ì¶”ì¶œ ì‹¤íŒ¨ â†’ ê±´ë„ˆëœ€")
            continue

        info = {
            "rank": rank,
            "title": title,
            "price_text": price_raw,
            "asin": asin,
            "url": href,
            "lg_match": lg_match
        }
        logging.info(f"CARD_DATA {json.dumps(info, ensure_ascii=False)}")

        if lg_match:
            parsed_items.append({
                "asin": asin,
                "title": title,
                "url": href,
                "price": price_raw,
                "rank": rank
            })

    return parsed_items

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê°€ê²© íŒŒì‹± í•¨ìˆ˜ ì¶”ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def price_to_float(txt: str):
    if not txt or pd.isna(txt):
        return np.nan
    txt = re.sub(r"[^\d,.,,]", "", txt)
    if txt.count(",") == 1 and txt.count(".") >= 1:
        txt = txt.replace(".", "").replace(",", ".")
    elif txt.count(",") == 1 and txt.count(".") == 0:
        txt = txt.replace(",", ".")
    try:
        return float(txt)
    except:
        return np.nan

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. ë©”ì¸ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
driver = get_driver()
wait = WebDriverWait(driver, 20)

# (A) ë°°ì†¡ì§€ UI ì„¤ì •
logging.info("ğŸ“ ë°°ì†¡ì§€ ì„¤ì • ì‹œì‘")
driver.get(BASE_URL)
time.sleep(2)
driver.add_cookie({"name": "lc-main",    "value": "de_DE"})
driver.add_cookie({"name": "i18n-prefs", "value": "EUR"})
driver.refresh()
time.sleep(5)

MAX_ATTEMPTS = 5
RETRY_DELAY  = 5

for attempt in range(1, MAX_ATTEMPTS + 1):
    try:
        logging.info(f"ğŸ” ë°°ì†¡ì§€ ë²„íŠ¼ ì°¾ê¸° ì‹œë„ {attempt}/{MAX_ATTEMPTS}")
        deliver_to_btn = wait.until(EC.presence_of_element_located(
            (By.XPATH, '//a[contains(@id, "nav-global-location")]')
        ))
        driver.execute_script("arguments[0].click();", deliver_to_btn)
        logging.info("ğŸ“ ë°°ì†¡ì§€ ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
        break
    except TimeoutException:
        logging.warning(f"âš ï¸ ë°°ì†¡ì§€ ë²„íŠ¼ ì‹¤íŒ¨ â€” ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„")
        driver.refresh()
        time.sleep(RETRY_DELAY)
else:
    logging.error(f"âŒ ë°°ì†¡ì§€ ë²„íŠ¼ ì‹¤íŒ¨ â†’ ì¢…ë£Œ")
    driver.quit()
    sys.exit(1)

try:
    zip_in = wait.until(EC.presence_of_element_located((By.ID, "GLUXZipUpdateInput")))
    zip_in.clear()
    zip_in.send_keys("65760")
    wait.until(EC.element_to_be_clickable((By.ID, "GLUXZipUpdate"))).click()
    logging.info("ğŸ“¦ ìš°í¸ë²ˆí˜¸ ì ìš© ì™„ë£Œ")
    time.sleep(3)
    driver.refresh()
    time.sleep(2)
except Exception as e:
    logging.error(f"âŒ ìš°í¸ë²ˆí˜¸ ì„¤ì • ì‹¤íŒ¨: {e}")
    driver.quit()
    sys.exit(1)

try:
    ship_to = wait.until(EC.presence_of_element_located((By.ID, "glow-ingress-line2"))).text
    logging.info(f"âœ… í˜„ì¬ ë°°ì†¡ì§€: {ship_to}")
except Exception:
    logging.error("âŒ ë°°ì†¡ì§€ í™•ì¸ ì‹¤íŒ¨")
    driver.quit()
    sys.exit(1)

# (B) í¬ë¡¤ë§
logging.info("ğŸ” í¬ë¡¤ë§ ì‹œì‘")
items = []
for pg in (1, 2):
    try:
        items += fetch_cards_and_parse(pg, driver)
    except TimeoutException:
        logging.error(f"â›” page {pg}: ì¹´ë“œ ë¡œë”© ì‹¤íŒ¨")
driver.quit()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. ì‹œíŠ¸ ê¸°ë¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cols = ["asin","title","rank","price","url","date","rank_delta","price_delta"]
df = pd.DataFrame(items)
if df.empty:
    logging.info("LG ëª¨ë‹ˆí„° ì—†ìŒ â†’ ì—…ë°ì´íŠ¸ ìƒëµ")
    sys.exit(0)

df = df.sort_values("rank").reset_index(drop=True)
kst = pytz.timezone("Asia/Seoul")
df["date"] = datetime.datetime.now(kst).strftime("%Y-%m-%d %H:%M:%S")

SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]
creds = Credentials.from_service_account_info(
    json.loads(base64.b64decode(os.environ["GCP_SA_BASE64"]).decode()),
    scopes=SCOPES
)
gc = gspread.authorize(creds)
sh = gc.open_by_key(os.environ["SHEET_ID"])

ws_hist = sh.worksheet("History") if "History" in [w.title for w in sh.worksheets()] else sh.add_worksheet("History", 2000, 20)
ws_today = sh.worksheet("Today") if "Today" in [w.title for w in sh.worksheets()] else sh.add_worksheet("Today", 100, 20)

try:
    prev = pd.DataFrame(ws_hist.get_all_records()).dropna()
except:
    prev = pd.DataFrame()

if not prev.empty and {"asin","rank","price","date"} <= set(prev.columns):
    last = prev.sort_values("date").groupby("asin", as_index=False).last()[["asin","rank","price"]]
    last.columns = ["asin","rank_prev","price_prev"]
    df = df.merge(last, on="asin", how="left")
else:
    df["rank_prev"] = None
    df["price_prev"] = None

df["price_curr_val"] = df["price"].apply(price_to_float)
df["price_prev_val"] = df["price_prev"].apply(price_to_float)

def calc_price_delta(row):
    cur, prev = row["price_curr_val"], row["price_prev_val"]
    if pd.isna(prev) or pd.isna(cur) or cur == prev:
        return "-"
    sign = "â–²" if cur > prev else "â–¼"
    return f"{sign}{abs(cur - prev):.2f}"

df["rank_delta"] = df["rank_prev"].combine(
    df["rank"],
    lambda prev, curr: "-" if pd.isna(prev) or int(prev) == int(curr)
                       else f"{'â–²' if prev > curr else 'â–¼'}{abs(int(prev - curr))}"
)
df["price_delta"] = df.apply(calc_price_delta, axis=1)

out_cols = ["asin","title","rank","price","url","date","rank_delta","price_delta"]
df_out   = df[out_cols].fillna("")

if not ws_hist.get_all_values():
    ws_hist.append_row(out_cols, value_input_option="USER_ENTERED")
ws_hist.append_rows(df_out.values.tolist(), value_input_option="USER_ENTERED")
ws_today.clear()
ws_today.update([out_cols] + df_out.values.tolist(), value_input_option="USER_ENTERED")

RED, BLUE = Color(1,0,0), Color(0,0,1)
fmt_ranges = []
for i, row in df_out.iterrows():
    r = i + 2
    for col, letter in [("rank_delta","G"),("price_delta","H")]:
        v = row[col]
        if v.startswith("â–²"):
            fmt_ranges.append((f"{letter}{r}", CellFormat(textFormat=TextFormat(bold=True, foregroundColor=RED))))
        elif v.startswith("â–¼"):
            fmt_ranges.append((f"{letter}{r}", CellFormat(textFormat=TextFormat(bold=True, foregroundColor=BLUE))))
if fmt_ranges:
    format_cell_ranges(ws_today, fmt_ranges)

logging.info("âœ… Google Sheet ì—…ë°ì´íŠ¸ ì™„ë£Œ â€” LG ëª¨ë‹ˆí„° %dê°œ", len(df_out))
